# -*- coding: utf-8 -*-
# AlphaZero 点格棋主配置文件
# 采用 Hydra 配置管理（与 GMD 一致）

defaults:
  - _self_
  - game: dots_and_boxes
  - model: transformer  # 切换到 Transformer (原来是 resnet)
  - trainer: alphazero

# 运行模式
mode: "train"  # train, eval, play, arena

# 硬件配置
cuda: true
gpu_ids: "0"
seed: 42

# 训练配置
num_iterations: 1000        # AlphaZero 总迭代次数
checkpoint_interval: 10     # 每隔多少次迭代保存模型
eval_interval: 5            # 每隔多少次迭代评估模型

# 自我对弈配置
num_self_play_games: 100    # 每次迭代的自我对弈局数
num_parallel_games: 8       # 并行对弈的游戏数
temperature_threshold: 15   # 前N步使用温度采样，之后贪心

# MCTS 配置
num_simulations: 800        # Transformer 强大，增加搜索深度 (100→800)
cpuct: 1.0                  # UCB 探索常数
dirichlet_alpha: 0.3        # Dirichlet 噪声参数
dirichlet_epsilon: 0.25     # 添加噪声的比例

# 神经网络训练配置
batch_size: 512             # Transformer 用更大batch (256→512, 5090够用)
learning_rate: 0.001
weight_decay: 1e-4
train_epochs: 10            # 增加训练轮数 (1→10)
replay_buffer_size: 200000  # 增大经验池 (20000→200000)

# 混合精度训练（节省显存，加速训练）
use_amp: true

# 路径配置
save_to: "results/dots_and_boxes"
checkname: "alphazero_6x6"

# 日志配置
log_interval: 10            # 每隔多少局游戏记录一次
tensorboard: true

# 评估配置
num_eval_games: 50          # 评估时的对局数
eval_against: "random"      # random, mcts_baseline, previous

# Arena 配置（模型对战）
arena_mode: "gpu_thread"    # gpu_thread（GPU+多线程，推荐）, multiprocess（CPU多进程）, batch（实验性）
arena_games: 100            # Arena 对战局数
arena_threshold: 0.55       # 新模型胜率阈值（超过才替换旧模型）
arena_num_workers: 6        # Arena 并行度（gpu_thread模式下为线程数）
arena_mcts_simulations: 200 # Arena MCTS 模拟次数（高精度评估）

# 可视化
visualize: false
save_game_records: true

# 分布式训练（可选）
distributed: false
world_size: 1
