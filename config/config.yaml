# -*- coding: utf-8 -*-
# AlphaZero 点格棋主配置文件
# 采用 Hydra 配置管理

defaults:
  - _self_
  - game: dots_and_boxes
  - model: resnet  # 使用ResNet (更快更稳定)
  - trainer: alphazero

# 运行模式
mode: "train"  # train, eval, play, arena

# 硬件配置
cuda: true
gpu_ids: "0"
seed: 42

# 训练配置
num_iterations: 200        # AlphaZero 总迭代次数（适合当前算力）
checkpoint_interval: 30     # 每隔多少次迭代保存模型
eval_interval: 5            # 每隔多少次迭代进行 Arena 对战评估（控制 arena_interval）

# 自我对弈配置
num_self_play_games: 200    # 每次迭代的自我对弈局数（增加工作量）
num_parallel_games: 30      # 并行对弈的游戏数（12→16，更多worker）
temperature_threshold: 30   # 前30步使用温度采样（AlphaZero标准），之后贪心

# MCTS 配置
num_simulations: 1200       # MCTS 模拟次数（800→1200，提高策略质量，降低策略损失）
cpuct: 1.25                  # UCB 探索常数
dirichlet_alpha: 0.3        # Dirichlet 噪声参数
dirichlet_epsilon: 0.25     # 添加噪声的比例

# 神经网络训练配置
batch_size: 4096            # 从7200降低到4096（提高泛化能力，避免过拟合）
learning_rate: 0.001        # AlphaZero标准lr（SGD+grad_clip=2.0可以稳定训练）
optimizer: "sgd"            # 使用SGD+Momentum（AlphaZero标准配置）
momentum: 0.9               # SGD动量参数（AlphaZero论文标准）
nesterov: true              # 使用Nesterov动量
weight_decay: 1e-4
train_epochs: 20             # AlphaZero标准:每样本训练~4次
replay_buffer_size: 60000   # 保留10次迭代,平衡新鲜度和稳定性

# 早停机制 (防止过度训练浪费时间)
patience: 5                 # epochs少了,early stopping也要快
min_delta: 0.0001           # 损失改进阈值

# 梯度控制（解决梯度爆炸）
grad_clip: 2.0              # 从5.0降低到2.0（更严格的梯度裁剪）

# 训练恢复
resume: false               # 从头开始训练（旧checkpoint包含错误配置）
resume_file: "latest.pth"   # 恢复用的检查点文件

# 并行优化参数
use_parallel: true
parallel_mode: "full"       # full=完全并行GPU / shared=共享GPU / single=单进程
use_multiprocess: true      # 使用多进程（兼容旧配置）
use_gpu_inference: true     # GPU 加速推理（自我对弈时也用 GPU）
mcts_batch_size: 380       # GPU 批量推理大小（32→64，增加批量）

# 混合精度训练（节省显存，加速训练）
use_amp: true

# 路径配置
save_to: "results/dots_and_boxes"
checkname: "alphazero_6x6"

# 日志配置
log_interval: 20            # 每隔多少局游戏记录一次
tensorboard: true

# Arena 配置（模型对战）
arena_mode: "gpu_parallel"  # gpu_parallel（GPU多进程，最快）, serial（串行）
arena_compare: 20           # Arena 对战局数（从8增加到40，提高评估准确性）
arena_threshold: 0.55       # 新模型胜率阈值（超过才替换旧模型）
arena_num_workers: 20       # Arena 并行进程数
arena_mcts_simulations: 1200 # Arena MCTS 模拟次数（1.5倍于训练，AlphaZero用2倍但我们算力有限）
update_threshold: 0.55      # 更新阈值（兼容旧参数名）

# 可视化
visualize: false
save_game_records: true

# 分布式训练（可选）
distributed: false
world_size: 1
