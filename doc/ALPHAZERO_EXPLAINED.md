# 🧠 AlphaZero 实现说明

## ✅ 这是真正的 AlphaZero 实现

### 核心组件对比

| 组件 | AlphaZero 论文 | 本项目实现 | 状态 |
|------|---------------|----------|------|
| **神经网络** | ResNet + 双头输出 | ✅ `model.py` | 完整 |
| **MCTS** | UCB搜索 + 神经网络指导 | ✅ `mcts.py` | 完整 |
| **自我对弈** | 生成训练数据 | ✅ `coach_alphazero.py` | 完整 |
| **Arena对战** | 新模型 vs 旧模型 | ✅ `arena.py` | **新增** |
| **模型筛选** | 胜率>55%才接受 | ✅ `coach_alphazero.py` | **新增** |
| **多进程加速** | 并行自我对弈 | ✅ 6进程并行 | 完整 |

---

## 🔄 AlphaZero 完整训练流程

```
for 每次迭代 (1-20次):
    
    1️⃣ 自我对弈 (40局)
       ├─ 使用当前最佳模型
       ├─ MCTS搜索 (25次模拟/步)
       ├─ 添加Dirichlet噪声增加探索
       └─ 收集 (状态, 策略, 结果) 训练数据
    
    2️⃣ 训练神经网络
       ├─ 使用收集的训练数据
       ├─ 优化策略损失 + 价值损失
       ├─ 20个epoch, batch=512
       └─ 得到"新模型"
    
    3️⃣ Arena 对战验证 ⭐ (AlphaZero 关键)
       ├─ 新模型 vs 旧模型
       ├─ 对战 20 局
       ├─ 交替先后手保证公平
       └─ 统计胜率
    
    4️⃣ 模型更新判断 ⭐ (AlphaZero 关键)
       ├─ 如果 新模型胜率 >= 55%
       │   └─ ✅ 接受新模型，更新最佳模型
       └─ 如果 新模型胜率 < 55%
           └─ ❌ 拒绝新模型，继续使用旧模型
```

---

## 📂 文件说明

### 核心训练文件

| 文件 | 用途 | 是否包含Arena |
|------|------|--------------|
| `train_alphazero.py` | ✅ **推荐** - 完整AlphaZero训练 | ✅ 是 |
| `extreme_train.py` | ⚡ 快速训练（无验证） | ❌ 否 |
| `quick_train.py` | 🧪 环境测试 | ❌ 否 |

### 核心模块

| 文件 | 功能 |
|------|------|
| `coach_alphazero.py` | ✅ 完整训练流程（含Arena） |
| `coach_parallel.py` | ⚡ 仅自我对弈+训练（无Arena） |
| `arena.py` | 🥊 模型对战评估器 |
| `mcts.py` | 🌲 蒙特卡洛树搜索 |
| `model.py` | 🧠 ResNet神经网络 |
| `game.py` | 🎮 点格棋游戏封装 |

### 评估工具

| 文件 | 功能 |
|------|------|
| `evaluate_model.py` | 🔬 模型效果评估 |
| `play.py` | 🎮 人机对战 |
| `test_project.py` | 🧪 环境测试 |

---

## 🚀 如何训练

### 方法1：完整 AlphaZero 训练（推荐）

```bash
cd /HFUT_002/DotsAndBoxes_AlphaZero
./start_training.sh
# 选择 1 - AlphaZero 完整训练
```

**或直接运行**:
```bash
/root/miniconda3/envs/gmd/bin/python train_alphazero.py
```

**训练输出示例**:
```
======================================================================
📍 AlphaZero 迭代 5/20
======================================================================

🎮 自我对弈(6进程): 100%|████████| 40/40 [01:20<00:00, 2.00s/it]
✓ 收集到 2400 个训练样本

🧠 训练神经网络: 20 epochs, 4 batches/epoch
  Epoch  1: Loss π=3.456 v=0.891 total=4.347 (4.2 batch/s)
  Epoch 10: Loss π=2.123 v=0.543 total=2.666 (4.5 batch/s)
  Epoch 20: Loss π=1.789 v=0.432 total=2.221 (4.5 batch/s)

======================================================================
🥊 Arena对战: 20 局 (player1 先手10局, 后手10局)
======================================================================
对战进度: 100%|████████| 20/20 [02:30<00:00, 7.5s/it]

======================================================================
📊 对战结果统计:
======================================================================
Player1 (新模型) 胜: 13/20 (65.0%)
Player2 (旧模型) 胜: 6/20 (30.0%)
平局:              1/20 (5.0%)
======================================================================

📊 新模型胜率: 65.0% (13胜 1平 6负)
✅ 新模型胜率 65.0% >= 55.0% → 接受更新!
🏆 最佳模型已保存: ./checkpoints/best_5.pth
```

---

## 🔬 如何验证训练效果

### 方法1：使用评估工具（推荐）

```bash
/root/miniconda3/envs/gmd/bin/python evaluate_model.py
```

**评估内容**:
1. **vs 随机策略** - 应该 >90% 胜率
2. **vs 贪心策略** - 应该 >70% 胜率  
3. **vs 早期模型** - 应该 >60% 胜率

**输出示例**:
```
========================================================================
📊 测试 1/3: 训练模型 vs 随机策略
========================================================================
期望结果: 模型应该 100% 胜率 (如果<90%说明训练失败)

对战进度: 100%|████████| 40/40 [01:30<00:00]

Player1 (新模型) 胜: 39/40 (97.5%)
Player2 (旧模型) 胜: 1/40 (2.5%)

结果评估:
  ✅ 优秀! 胜率 97.5% - 模型已学会基本策略

========================================================================
🏆 综合评估报告
========================================================================
性能指标:
  vs 随机策略: 97.5% ✅
  vs 贪心策略: 75.0% ✅
  vs 早期模型: 62.5% ✅

总体评分: 100%
评级: ⭐⭐⭐ 优秀 - 模型训练成功!
```

### 方法2：人机对战

```bash
/root/miniconda3/envs/gmd/bin/python play.py
```

亲自体验AI的水平！

---

## ⚖️ 两种训练模式对比

| 特性 | AlphaZero完整训练 | 快速训练(无验证) |
|------|------------------|-----------------|
| **Arena对战** | ✅ 每次迭代都验证 | ❌ 无 |
| **模型筛选** | ✅ 只接受强的模型 | ❌ 无，可能退化 |
| **训练稳定性** | ✅ 高，保证单调提升 | ⚠️ 低，可能过拟合 |
| **训练速度** | 慢 (~15分钟/迭代) | 快 (~3分钟/迭代) |
| **GPU利用率** | 中 (~50%) | 高 (~70%) |
| **推荐场景** | 🏆 正式训练 | 🧪 快速实验 |

---

## 🎯 训练建议

### 新手流程

```bash
# 1. 快速测试环境（2分钟）
python test_project.py

# 2. 开启GPU监控（新终端）
./monitor_gpu.sh

# 3. 启动完整训练（主终端）
python train_alphazero.py

# 4. 等待训练完成（约5小时 = 20迭代 × 15分钟）

# 5. 评估模型效果
python evaluate_model.py

# 6. 人机对战验证
python play.py
```

### 调整训练参数

编辑 `train_alphazero.py`:

```python
args = {
    'num_iterations': 20,      # 迭代次数（↑更多=更强）
    'num_episodes': 40,        # 每次对弈局数（↑更多=更稳定）
    'arena_compare': 20,       # Arena对战局数（↑更多=验证更准确）
    'update_threshold': 0.55,  # 接受阈值（0.55 = 55%胜率）
    'num_simulations': 25,     # MCTS模拟次数（↑更多=更强但更慢）
    'epochs': 20,              # 训练轮数（↓减少避免过拟合）
    'batch_size': 512,         # 批大小（↑增加提升GPU利用率）
}
```

---

## 🔍 为什么之前的实现不完整？

### 缺失的关键机制

之前的 `extreme_train.py` 和 `coach_parallel.py`:
- ✅ 有自我对弈
- ✅ 有神经网络训练
- ❌ **没有 Arena 对战**
- ❌ **没有模型筛选**

这导致：
1. **无法验证训练效果** - 不知道模型是否在变强
2. **可能过拟合** - 模型可能记住了自我对弈，但泛化能力差
3. **可能退化** - 新模型可能比旧模型更弱
4. **不符合 AlphaGo/AlphaZero 论文** - 缺少核心验证环节

### 现在的完整实现

新的 `train_alphazero.py` + `coach_alphazero.py` + `arena.py`:
- ✅ 自我对弈
- ✅ 神经网络训练
- ✅ **Arena 对战验证**
- ✅ **模型筛选机制**
- ✅ **训练效果可评估**

---

## 📊 预期训练效果

### 迭代1-5
- Arena胜率: 50-60% (新模型略优)
- vs随机: 60-80% (开始学会基本策略)

### 迭代6-10
- Arena胜率: 55-65% (稳定提升)
- vs随机: 85-95% (已掌握基本策略)

### 迭代11-20
- Arena胜率: 52-58% (微调，提升变慢)
- vs随机: >95% (完全掌握)
- vs贪心: 70-80% (超越简单策略)

---

## 🏆 总结

### ✅ 现在有了完整的 AlphaZero

- 神经网络 ✅
- MCTS ✅
- 自我对弈 ✅
- Arena对战 ✅ **（新增）**
- 模型筛选 ✅ **（新增）**
- 效果评估 ✅ **（新增）**
- 多进程加速 ✅

### 🚀 立即开始

```bash
cd /HFUT_002/DotsAndBoxes_AlphaZero
./start_training.sh
# 选择 1 - AlphaZero 完整训练
```

训练完成后：
```bash
# 评估效果
python evaluate_model.py

# 人机对战
python play.py
```

---

**现在这才是真正的 AlphaZero！** 🎉
