# -*- coding: utf-8 -*-
"""
BaseCoach - AlphaZero è®­ç»ƒåŸºç±»

è®¾è®¡ç†å¿µï¼š
1. æå– Coach å’Œ ParallelCoach çš„å…¬å…±é€»è¾‘
2. ç®€åŒ–æ¨¡å‹æ›´æ–°æœºåˆ¶ï¼ˆprevious_net vs current_netï¼‰
3. ç»Ÿä¸€é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
"""

import os
import numpy as np
import torch
import torch.optim as optim
import torch.cuda.amp as amp
from tqdm import tqdm
from collections import deque
from abc import ABC, abstractmethod
import copy


class BaseCoach(ABC):
    """
    AlphaZero è®­ç»ƒåŸºç±»
    
    å­ç±»éœ€è¦å®ç°ï¼š
    - execute_episode() æˆ– execute_episode_batch(): è‡ªæˆ‘å¯¹å¼ˆé€»è¾‘
    """
    
    def __init__(self, game, nnet, args):
        """
        åˆå§‹åŒ–æ•™ç»ƒ
        
        Args:
            game: æ¸¸æˆç¯å¢ƒ
            nnet: ç¥ç»ç½‘ç»œæ¨¡å‹
            args: é…ç½®å‚æ•°å­—å…¸
        """
        self.game = game
        self.nnet = nnet  # å½“å‰æ¨¡å‹ (current_net)
        self.args = args
        
        # è®­ç»ƒå†å²
        self.train_examples_history = deque(
            maxlen=args.get('num_iters_for_train_examples_history', 20)
        )
        
        # å‰ä¸€ä¸ªæ¨¡å‹ï¼ˆç”¨äº Arena å¯¹æˆ˜ï¼‰
        self.previous_nnet = None
        
        # åˆå§‹åŒ–æ—¶ï¼Œå°†å½“å‰æ¨¡å‹ä½œä¸º previous_net
        self._initialize_previous_net()
    
    def _initialize_previous_net(self):
        """åˆå§‹åŒ– previous_net ä¸ºå½“å‰æ¨¡å‹çš„å‰¯æœ¬"""
        self.previous_nnet = copy.deepcopy(self.nnet)
        print("âœ“ åˆå§‹åŒ– previous_net = current_net")
    
    @abstractmethod
    def execute_episode(self):
        """
        æ‰§è¡Œä¸€å±€è‡ªæˆ‘å¯¹å¼ˆï¼ˆå•è¿›ç¨‹ç‰ˆæœ¬ï¼‰
        
        Returns:
            è®­ç»ƒæ ·æœ¬åˆ—è¡¨ [(observation, policy, value), ...]
        """
        pass
    
    def collect_self_play_data(self):
        """
        æ”¶é›†è‡ªæˆ‘å¯¹å¼ˆæ•°æ®
        
        æ³¨æ„ï¼šå­ç±»å¯ä»¥é‡å†™æ­¤æ–¹æ³•ä»¥å®ç°å¹¶è¡Œç‰ˆæœ¬
        
        Returns:
            è®­ç»ƒæ ·æœ¬åˆ—è¡¨
        """
        iteration_train_examples = []
        
        for _ in tqdm(range(self.args['num_episodes']), desc="è‡ªæˆ‘å¯¹å¼ˆ"):
            iteration_train_examples.extend(self.execute_episode())
        
        return iteration_train_examples
    
    def learn(self):
        """
        AlphaZero ä¸»è®­ç»ƒå¾ªç¯
        
        æµç¨‹ï¼š
        1. è‡ªæˆ‘å¯¹å¼ˆç”Ÿæˆè®­ç»ƒæ•°æ®
        2. è®­ç»ƒå½“å‰æ¨¡å‹
        3. Arena å¯¹æˆ˜ï¼ˆcurrent_net vs previous_netï¼‰
        4. æ ¹æ®èƒœç‡å†³å®šæ˜¯å¦æ›´æ–° previous_net
        5. ä¿å­˜æ£€æŸ¥ç‚¹
        """
        for iteration in range(1, self.args['num_iterations'] + 1):
            print(f'\n{"=" * 60}')
            print(f'è¿­ä»£ {iteration}/{self.args["num_iterations"]}')
            print(f'{"=" * 60}')
            
            # ========== 1. è‡ªæˆ‘å¯¹å¼ˆ ==========
            print(f'\n[1/4] è‡ªæˆ‘å¯¹å¼ˆ...')
            iteration_train_examples = self.collect_self_play_data()
            
            print(f'âœ“ ç”Ÿæˆ {len(iteration_train_examples)} ä¸ªè®­ç»ƒæ ·æœ¬')
            
            # æ·»åŠ åˆ°å†å²
            self.train_examples_history.append(iteration_train_examples)
            
            # åˆå¹¶æ‰€æœ‰å†å²æ•°æ®
            train_examples = []
            for examples in self.train_examples_history:
                train_examples.extend(examples)
            
            print(f'âœ“ è®­ç»ƒé›†å¤§å°: {len(train_examples)} æ ·æœ¬')
            
            # ========== 2. ä¿å­˜è®­ç»ƒå‰çš„æ¨¡å‹ ==========
            print(f'\n[2/4] ä¿å­˜å½“å‰æ¨¡å‹...')
            self.save_checkpoint(filename='temp.pth')
            
            # ========== 3. è®­ç»ƒç¥ç»ç½‘ç»œ ==========
            print(f'\n[3/4] è®­ç»ƒç¥ç»ç½‘ç»œ...')
            try:
                train_stats = self.train(train_examples)
                print(f'âœ“ è®­ç»ƒå®Œæˆ')
                if train_stats:
                    print(f'  å¹³å‡ç­–ç•¥æŸå¤±: {np.mean(train_stats["pi_losses"]):.4f}')
                    print(f'  å¹³å‡ä»·å€¼æŸå¤±: {np.mean(train_stats["v_losses"]):.4f}')
            except Exception as e:
                print(f'âŒ è®­ç»ƒå‡ºé”™: {e}')
                import traceback
                traceback.print_exc()
                # æ¢å¤åˆ°è®­ç»ƒå‰çš„æ¨¡å‹
                self._load_temp_checkpoint()
                continue
            
            # ========== 4. Arena å¯¹æˆ˜ ==========
            arena_interval = self.args.get('arena_interval', 1)
            
            if iteration % arena_interval == 0:
                print(f'\n[4/4] Arena å¯¹æˆ˜éªŒè¯...')
                try:
                    should_accept = self._arena_compare()
                    
                    if should_accept:
                        print('âœ… æ–°æ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œæ›´æ–° previous_net')
                        self._accept_new_model()
                        
                        # ä¿å­˜æœ€ä½³æ¨¡å‹
                        self.save_checkpoint(filename=f'best_{iteration}.pth')
                    else:
                        print('âŒ æ–°æ¨¡å‹è¡¨ç°ä¸ä½³ï¼Œä¿æŒ previous_netï¼Œå›é€€ current_net')
                        self._reject_new_model()
                
                except Exception as e:
                    print(f'âŒ Arena å¯¹æˆ˜å‡ºé”™: {e}')
                    import traceback
                    traceback.print_exc()
                    # å‡ºé”™æ—¶ä¿å®ˆç­–ç•¥ï¼šä¸æ¥å—æ–°æ¨¡å‹
                    self._reject_new_model()
            else:
                print(f'\n[4/4] è·³è¿‡ Arena éªŒè¯ (æ¯ {arena_interval} æ¬¡è¿­ä»£éªŒè¯ä¸€æ¬¡)')
                # ä¸éªŒè¯æ—¶ï¼Œä¿å®ˆç­–ç•¥ï¼šæ¥å—æ–°æ¨¡å‹
                self._accept_new_model()
            
            # ========== 5. ä¿å­˜æ£€æŸ¥ç‚¹ ==========
            if iteration % self.args.get('checkpoint_interval', 10) == 0:
                self.save_checkpoint(filename=f'checkpoint_{iteration}.pth')
            
            self.save_checkpoint(filename='latest.pth')
            
            print(f'\n{"=" * 60}')
            print(f'è¿­ä»£ {iteration} å®Œæˆ')
            print(f'{"=" * 60}\n')
    
    def _arena_compare(self):
        """
        Arena å¯¹æˆ˜ï¼šcurrent_net vs previous_net
        
        Returns:
            bool: æ˜¯å¦åº”è¯¥æ¥å—æ–°æ¨¡å‹
        """
        from .arena import compare_models
        
        win_rate, should_accept = compare_models(
            self.game,
            self.nnet,  # current_net
            self.previous_nnet,  # previous_net
            self.args
        )
        
        print(f'  Arena èƒœç‡: {win_rate * 100:.1f}%')
        
        return should_accept
    
    def _accept_new_model(self):
        """æ¥å—æ–°æ¨¡å‹ï¼šç”¨ current_net æ›´æ–° previous_net"""
        self.previous_nnet = copy.deepcopy(self.nnet)
    
    def _reject_new_model(self):
        """æ‹’ç»æ–°æ¨¡å‹ï¼šç”¨ previous_net æ¢å¤ current_net"""
        if self.previous_nnet is not None:
            self.nnet.load_state_dict(self.previous_nnet.state_dict())
    
    def _load_temp_checkpoint(self):
        """åŠ è½½ä¸´æ—¶æ£€æŸ¥ç‚¹"""
        temp_path = os.path.join(self.args['checkpoint'], 'temp.pth')
        if os.path.exists(temp_path):
            checkpoint = torch.load(temp_path)
            self.nnet.load_state_dict(checkpoint['state_dict'])
            print(f'âœ“ å·²æ¢å¤åˆ°è®­ç»ƒå‰çš„æ¨¡å‹')
    
    def train(self, examples):
        """
        è®­ç»ƒç¥ç»ç½‘ç»œ
        
        Args:
            examples: è®­ç»ƒæ ·æœ¬åˆ—è¡¨ [(observation, policy, value), ...]
        
        Returns:
            dict: è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        """
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = optim.Adam(
            self.nnet.parameters(),
            lr=self.args['lr'],
            weight_decay=self.args.get('weight_decay', 1e-4)
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=self.args['epochs'],
            eta_min=self.args['lr'] * 0.01
        )
        
        # æ··åˆç²¾åº¦è®­ç»ƒ
        use_amp = self.args.get('use_amp', False) and self.args.get('cuda', False)
        scaler = amp.GradScaler(enabled=use_amp) if use_amp else None
        
        self.nnet.train()
        
        pi_losses = []
        v_losses = []
        total_losses = []
        
        for epoch in range(self.args['epochs']):
            current_lr = optimizer.param_groups[0]['lr']
            print(f'Epoch {epoch + 1}/{self.args["epochs"]} - LR: {current_lr:.6f}')
            
            # æ‰“ä¹±æ•°æ®
            np.random.shuffle(examples)
            num_batches = max(1, len(examples) // self.args['batch_size'])
            
            epoch_pi_loss = 0
            epoch_v_loss = 0
            epoch_total_loss = 0
            
            batch_iterator = tqdm(range(num_batches), desc='Training')
            
            for batch_idx in batch_iterator:
                try:
                    # é‡‡æ · batch
                    sample_ids = np.random.randint(len(examples), size=self.args['batch_size'])
                    boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))
                    
                    # è½¬æ¢ä¸º Tensor
                    boards = torch.FloatTensor(np.array(boards))
                    target_pis = torch.FloatTensor(np.array(pis))
                    target_vs = torch.FloatTensor(np.array(vs))
                    
                    if self.args.get('cuda', False):
                        boards = boards.cuda()
                        target_pis = target_pis.cuda()
                        target_vs = target_vs.cuda()
                    
                    # å‰å‘ä¼ æ’­
                    optimizer.zero_grad()
                    
                    if use_amp:
                        with amp.autocast(enabled=True):
                            out_pi, out_v = self.nnet(boards)
                            
                            # ç­–ç•¥æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰
                            l_pi = -torch.sum(target_pis * out_pi) / target_pis.size(0)
                            
                            # ä»·å€¼æŸå¤±ï¼ˆMSEï¼‰
                            l_v = torch.sum((target_vs - out_v.view(-1)) ** 2) / target_vs.size(0)
                            
                            # æ€»æŸå¤±
                            total_loss = l_pi + l_v
                        
                        # åå‘ä¼ æ’­ï¼ˆæ··åˆç²¾åº¦ï¼‰
                        scaler.scale(total_loss).backward()
                        
                        # æ¢¯åº¦è£å‰ª
                        scaler.unscale_(optimizer)
                        torch.nn.utils.clip_grad_norm_(
                            self.nnet.parameters(),
                            self.args.get('grad_clip', 5.0)
                        )
                        
                        # ä¼˜åŒ–å™¨æ­¥è¿›
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        # æ ‡å‡†è®­ç»ƒ
                        out_pi, out_v = self.nnet(boards)
                        
                        # ç­–ç•¥æŸå¤±ï¼ˆäº¤å‰ç†µï¼‰
                        l_pi = -torch.sum(target_pis * out_pi) / target_pis.size(0)
                        
                        # ä»·å€¼æŸå¤±ï¼ˆMSEï¼‰
                        l_v = torch.sum((target_vs - out_v.view(-1)) ** 2) / target_vs.size(0)
                        
                        # æ€»æŸå¤±
                        total_loss = l_pi + l_v
                        
                        # åå‘ä¼ æ’­
                        total_loss.backward()
                        
                        # æ¢¯åº¦è£å‰ª
                        torch.nn.utils.clip_grad_norm_(
                            self.nnet.parameters(),
                            self.args.get('grad_clip', 5.0)
                        )
                        
                        # ä¼˜åŒ–å™¨æ­¥è¿›
                        optimizer.step()
                    
                    # è®°å½•æŸå¤±
                    epoch_pi_loss += l_pi.item()
                    epoch_v_loss += l_v.item()
                    epoch_total_loss += total_loss.item()
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    batch_iterator.set_postfix({
                        'pi_loss': f'{l_pi.item():.4f}',
                        'v_loss': f'{l_v.item():.4f}',
                        'total': f'{total_loss.item():.4f}'
                    })
                
                except Exception as e:
                    print(f'\nâŒ è®­ç»ƒæ‰¹æ¬¡ {batch_idx} å‡ºé”™: {e}')
                    import traceback
                    traceback.print_exc()
                    continue
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step()
            
            # è®°å½• epoch å¹³å‡æŸå¤±
            if num_batches > 0:
                avg_pi_loss = epoch_pi_loss / num_batches
                avg_v_loss = epoch_v_loss / num_batches
                avg_total_loss = epoch_total_loss / num_batches
                
                pi_losses.append(avg_pi_loss)
                v_losses.append(avg_v_loss)
                total_losses.append(avg_total_loss)
                
                print(f'Epoch {epoch + 1} å¹³å‡æŸå¤± - '
                      f'Policy: {avg_pi_loss:.4f}, '
                      f'Value: {avg_v_loss:.4f}, '
                      f'Total: {avg_total_loss:.4f}')
        
        # è¿”å›è®­ç»ƒç»Ÿè®¡
        return {
            'pi_losses': pi_losses,
            'v_losses': v_losses,
            'total_losses': total_losses
        }
    
    def save_checkpoint(self, filename='checkpoint.pth'):
        """
        ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
        
        Args:
            filename: æ–‡ä»¶å
        """
        checkpoint_dir = self.args.get('checkpoint', './checkpoints')
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        filepath = os.path.join(checkpoint_dir, filename)
        
        torch.save({
            'state_dict': self.nnet.state_dict(),
            'args': self.args
        }, filepath)
        
        # åªåœ¨ä¿å­˜é‡è¦æ£€æŸ¥ç‚¹æ—¶æ‰“å°
        if 'best' in filename or 'checkpoint' in filename:
            print(f'ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {filepath}')
    
    def load_checkpoint(self, filename='checkpoint.pth'):
        """
        åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹
        
        Args:
            filename: æ–‡ä»¶å
        """
        checkpoint_dir = self.args.get('checkpoint', './checkpoints')
        filepath = os.path.join(checkpoint_dir, filename)
        
        if os.path.exists(filepath):
            checkpoint = torch.load(filepath)
            self.nnet.load_state_dict(checkpoint['state_dict'])
            print(f'âœ“ å·²åŠ è½½æ¨¡å‹: {filepath}')
            return True
        else:
            print(f'âš ï¸  æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {filepath}')
            return False
