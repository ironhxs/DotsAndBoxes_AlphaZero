# -*- coding: utf-8 -*-
"""AlphaZero å®Œæ•´è®­ç»ƒæ•™ç»ƒ - åŒ…å«Arenaå¯¹æˆ˜éªŒè¯"""

import warnings
warnings.filterwarnings('ignore', category=UserWarning)

import os
import time
import copy
import numpy as np
from collections import deque
from tqdm import tqdm
import torch
import torch.optim as optim
from .mcts import MCTS
from .arena import Arena
from multiprocessing import Pool, cpu_count


class Coach:
    """
    çœŸæ­£çš„AlphaZeroè®­ç»ƒæµç¨‹:
    1. è‡ªæˆ‘å¯¹å¼ˆæ”¶é›†æ•°æ®
    2. è®­ç»ƒç¥ç»ç½‘ç»œå¾—åˆ°æ–°æ¨¡å‹
    3. Arenaå¯¹æˆ˜: æ–°æ¨¡å‹ vs æ—§æ¨¡å‹
    4. åªæœ‰æ–°æ¨¡å‹èƒœç‡ > é˜ˆå€¼(55%) æ‰æ¥å—æ›´æ–°
    """
    
    def __init__(self, game, nnet, args):
        self.game = game
        self.nnet = nnet
        self.args = args
        self.train_examples_history = []
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        self.best_nnet = copy.deepcopy(nnet)
        
    def execute_episode(self, _=None):
        """å•å±€æ¸¸æˆæ‰§è¡Œ (æ”¯æŒå¤šè¿›ç¨‹è°ƒç”¨)"""
        mcts = MCTS(self.game, self.nnet, self.args)
        train_examples = []
        state = self.game.get_initial_state()
        cur_player = 0
        episode_step = 0
        
        while True:
            episode_step += 1
            canonical_board = self.game.get_observation(state)
            temp = int(episode_step < self.args['temp_threshold'])
            pi = mcts.get_action_prob(state, temp=temp)
            
            # å‰30æ­¥æ·»åŠ Dirichletå™ªå£°å¢åŠ æ¢ç´¢
            if episode_step <= 30:
                noise = np.random.dirichlet([self.args['dirichlet_alpha']] * len(pi))
                pi = (1 - self.args['dirichlet_epsilon']) * pi + self.args['dirichlet_epsilon'] * noise
                pi = pi * self.game.get_valid_moves(state)
                pi = pi / np.sum(pi)
            
            train_examples.append([canonical_board, cur_player, pi, None])
            action = np.random.choice(len(pi), p=pi)
            state = self.game.get_next_state(state, action)
            r = self.game.get_game_result(state, cur_player)
            
            if r != 0:
                # æ¸¸æˆç»“æŸï¼Œä¸ºæ‰€æœ‰è®­ç»ƒæ ·æœ¬åˆ†é…å¥–åŠ±
                return [(x[0], x[2], r * ((-1) ** (x[1] != cur_player))) for x in train_examples]
            
            new_player = self.game.get_current_player(state)
            if new_player != cur_player:
                cur_player = new_player
    
    def learn(self):
        """
        AlphaZeroå®Œæ•´è®­ç»ƒå¾ªç¯:
        æ¯æ¬¡è¿­ä»£ = è‡ªæˆ‘å¯¹å¼ˆ â†’ è®­ç»ƒ â†’ Arenaå¯¹æˆ˜ â†’ æ¨¡å‹æ›´æ–°åˆ¤æ–­
        """
        # ç¡®å®šå¹¶è¡Œè¿›ç¨‹æ•°
        num_workers = self.args.get('num_workers', min(cpu_count() - 1, 8))
        use_parallel = self.args.get('use_parallel', True) and num_workers > 1
        
        if use_parallel:
            print(f"ğŸš€ å¯ç”¨å¤šè¿›ç¨‹å¹¶è¡Œ: {num_workers} ä¸ªå·¥ä½œè¿›ç¨‹")
        
        for i in range(1, self.args['num_iterations'] + 1):
            print(f'\n{"=" * 70}')
            print(f'ğŸ“ AlphaZero è¿­ä»£ {i}/{self.args["num_iterations"]}')
            print(f'{"=" * 70}\n')
            
            # ============================================================
            # é˜¶æ®µ1: è‡ªæˆ‘å¯¹å¼ˆæ”¶é›†è®­ç»ƒæ•°æ®
            # ============================================================
            iteration_train_examples = deque([], maxlen=self.args['max_queue_length'])
            
            if use_parallel:
                with Pool(processes=num_workers) as pool:
                    results = list(tqdm(
                        pool.imap(self.execute_episode, range(self.args['num_episodes'])),
                        total=self.args['num_episodes'],
                        desc=f"ğŸ® è‡ªæˆ‘å¯¹å¼ˆ({num_workers}è¿›ç¨‹)"
                    ))
                    for result in results:
                        iteration_train_examples += result
            else:
                for _ in tqdm(range(self.args['num_episodes']), desc="ğŸ® è‡ªæˆ‘å¯¹å¼ˆ"):
                    iteration_train_examples += self.execute_episode()
            
            self.train_examples_history.append(iteration_train_examples)
            if len(self.train_examples_history) > self.args['num_iters_for_train_examples_history']:
                self.train_examples_history.pop(0)
            
            train_examples = []
            for e in self.train_examples_history:
                train_examples.extend(e)
            
            print(f"âœ“ æ”¶é›†åˆ° {len(train_examples)} ä¸ªè®­ç»ƒæ ·æœ¬\n")
            
            # ============================================================
            # é˜¶æ®µ2: è®­ç»ƒç¥ç»ç½‘ç»œ
            # ============================================================
            # ä¿å­˜å½“å‰æ¨¡å‹ä½œä¸º"æ—§æ¨¡å‹"
            temp_nnet = copy.deepcopy(self.nnet)
            
            self.train(train_examples)
            
            # ============================================================
            # é˜¶æ®µ3: Arenaå¯¹æˆ˜ - æ–°æ¨¡å‹ vs æ—§æ¨¡å‹ (æ¯Næ¬¡è¿­ä»£è¿›è¡Œä¸€æ¬¡)
            # ============================================================
            arena_interval = self.args.get('arena_interval', 1)  # é»˜è®¤æ¯æ¬¡éƒ½éªŒè¯
            should_arena = (i % arena_interval == 0) or (i == self.args['num_iterations'])
            
            if should_arena:
                print(f"\nğŸ¥Š Arenaå¯¹æˆ˜éªŒè¯ (è¿­ä»£ {i}): æ–°æ¨¡å‹ vs æ—§æ¨¡å‹")
                
                arena = Arena(self.nnet, temp_nnet, self.game, self.args)
                new_wins, old_wins, draws = arena.play_games(self.args['arena_compare'])
                
                # è®¡ç®—æ–°æ¨¡å‹èƒœç‡
                total_games = new_wins + old_wins + draws
                new_win_rate = (new_wins + 0.5 * draws) / total_games
                
                print(f"\nğŸ“Š æ–°æ¨¡å‹èƒœç‡: {new_win_rate*100:.1f}% ({new_wins}èƒœ {draws}å¹³ {old_wins}è´Ÿ)")
                
                # ============================================================
                # é˜¶æ®µ4: æ¨¡å‹æ›´æ–°åˆ¤æ–­
                # ============================================================
                threshold = self.args.get('update_threshold', 0.55)
                
                if new_win_rate >= threshold:
                    print(f'âœ… æ–°æ¨¡å‹èƒœç‡ {new_win_rate*100:.1f}% >= {threshold*100:.1f}% â†’ æ¥å—æ›´æ–°!')
                    self.best_nnet = copy.deepcopy(self.nnet)
                    self.save_checkpoint(filename=f'best_{i}.pth')
                else:
                    print(f'âŒ æ–°æ¨¡å‹èƒœç‡ {new_win_rate*100:.1f}% < {threshold*100:.1f}% â†’ æ‹’ç»æ›´æ–°!')
                    print(f'   æ¢å¤ä½¿ç”¨æ—§æ¨¡å‹ç»§ç»­è®­ç»ƒ...')
                    self.nnet = copy.deepcopy(temp_nnet)
            else:
                print(f"\nâ­ï¸  è·³è¿‡ArenaéªŒè¯ (ä¸‹æ¬¡éªŒè¯: è¿­ä»£ {(i // arena_interval + 1) * arena_interval})")
                # ä¸éªŒè¯æ—¶ï¼Œç›´æ¥æ¥å—æ–°æ¨¡å‹
                self.best_nnet = copy.deepcopy(self.nnet)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if i % self.args['checkpoint_interval'] == 0:
                self.save_checkpoint(filename=f'checkpoint_{i}.pth')
            self.save_checkpoint(filename='latest.pth')
    
    def train(self, examples):
        """è®­ç»ƒç¥ç»ç½‘ç»œ"""
        optimizer = optim.Adam(
            self.nnet.parameters(), 
            lr=self.args['lr'], 
            weight_decay=self.args['weight_decay']
        )
        self.nnet.train()
        
        # é¢„å…ˆæ‰“ä¹±æ•°æ®
        np.random.shuffle(examples)
        num_batches = len(examples) // self.args['batch_size']
        all_indices = np.arange(len(examples))
        
        print(f"\nğŸ§  è®­ç»ƒç¥ç»ç½‘ç»œ: {self.args['epochs']} epochs, {num_batches} batches/epoch")
        
        for epoch in range(self.args['epochs']):
            np.random.shuffle(all_indices)
            
            epoch_pi_loss = 0
            epoch_v_loss = 0
            epoch_start = time.time()
            
            batch_iter = tqdm(range(num_batches), desc=f'  Epoch {epoch+1}/{self.args["epochs"]}', leave=False)
            
            for batch_idx in batch_iter:
                start_idx = batch_idx * self.args['batch_size']
                end_idx = start_idx + self.args['batch_size']
                batch_indices = all_indices[start_idx:end_idx]
                
                boards, pis, vs = list(zip(*[examples[i] for i in batch_indices]))
                
                boards = torch.FloatTensor(np.array(boards))
                target_pis = torch.FloatTensor(np.array(pis))
                target_vs = torch.FloatTensor(np.array(vs))
                
                if self.args['cuda']:
                    boards = boards.cuda()
                    target_pis = target_pis.cuda()
                    target_vs = target_vs.cuda()
                
                # å‰å‘ä¼ æ’­
                out_pi, out_v = self.nnet(boards)
                l_pi = -torch.sum(target_pis * out_pi) / target_pis.size()[0]
                l_v = torch.sum((target_vs - out_v.view(-1)) ** 2) / target_vs.size()[0]
                total_loss = l_pi + l_v
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.nnet.parameters(), max_norm=1.0)
                optimizer.step()
                
                epoch_pi_loss += l_pi.item()
                epoch_v_loss += l_v.item()
            
            # è¾“å‡ºæŸå¤±
            avg_pi_loss = epoch_pi_loss / num_batches
            avg_v_loss = epoch_v_loss / num_batches
            epoch_time = time.time() - epoch_start
            
            if (epoch + 1) % 10 == 0 or epoch == 0 or (epoch + 1) == self.args['epochs']:
                print(f'  Epoch {epoch+1:2d}: Loss Ï€={avg_pi_loss:.3f} v={avg_v_loss:.3f} '
                      f'total={avg_pi_loss+avg_v_loss:.3f} ({num_batches/epoch_time:.1f} batch/s)')
    
    def save_checkpoint(self, filename='checkpoint.pth'):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        os.makedirs(self.args['checkpoint'], exist_ok=True)
        filepath = os.path.join(self.args['checkpoint'], filename)
        torch.save({'state_dict': self.nnet.state_dict()}, filepath)
        if 'best' in filename:
            print(f'ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {filepath}')
        else:
            print(f'ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}')
