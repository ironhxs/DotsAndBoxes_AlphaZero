# -*- coding: utf-8 -*-
"""AlphaZero å®Œæ•´è®­ç»ƒæ•™ç»ƒ - åŒ…å«Arenaå¯¹æˆ˜éªŒè¯"""

import warnings
warnings.filterwarnings('ignore', category=UserWarning)

import os
import time
import copy
import numpy as np
from collections import deque
from tqdm import tqdm
import torch
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from .mcts import MCTS
from .arena import Arena
from multiprocessing import Pool, cpu_count


def _init_worker_cuda():
    """å­è¿›ç¨‹åˆå§‹åŒ–å‡½æ•° - è®¾ç½® CUDA ç¯å¢ƒ"""
    import torch
    if torch.cuda.is_available():
        # è§¦å‘ CUDA åˆå§‹åŒ–
        torch.cuda.current_device()
        # ç¦ç”¨ cudnn benchmark (å¤šè¿›ç¨‹ç¯å¢ƒä¸‹æ›´ç¨³å®š)
        torch.backends.cudnn.benchmark = False


def _execute_episode_worker(args_tuple):
    """å…¨å±€å‡½æ•°ç”¨äºå¤šè¿›ç¨‹ - é¿å…åºåˆ—åŒ– self"""
    game, nnet_state_dict, args, seed = args_tuple
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
    
    # é‡å»ºæ¨¡å‹ (æ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹)
    from .model_transformer import DotsAndBoxesTransformer
    nnet = DotsAndBoxesTransformer(
        game,
        num_filters=args['num_filters'],
        num_blocks=args['num_res_blocks'],
        num_heads=args['num_heads']
    )
    nnet.load_state_dict(nnet_state_dict)
    
    # âš¡ æ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹ä½¿ç”¨GPUæ¨ç†ï¼ˆå¿…é¡»ç”¨GPUï¼Œå¦åˆ™Transformerå¤ªæ…¢ï¼‰
    # æ³¨æ„ï¼šè¿›ç¨‹æ•°ä¸èƒ½å¤ªå¤šï¼Œå¦åˆ™ä¼šOOM
    if args.get('cuda', False) and torch.cuda.is_available():
        nnet = nnet.cuda()
    
    nnet.eval()
    
    # æ‰§è¡Œä¸€å±€æ¸¸æˆ
    mcts = MCTS(game, nnet, args)
    train_examples = []
    state = game.get_initial_state()
    cur_player = 0
    episode_step = 0
    
    while True:
        episode_step += 1
        canonical_board = game.get_observation(state)
        temp = int(episode_step < args['temp_threshold'])
        pi = mcts.get_action_prob(state, temp=temp)
        
        # å‰30æ­¥æ·»åŠ Dirichletå™ªå£°
        if episode_step <= 30:
            noise = np.random.dirichlet([args.get('dirichlet_alpha', 0.3)] * len(pi))
            pi = (1 - args.get('dirichlet_epsilon', 0.25)) * pi + args.get('dirichlet_epsilon', 0.25) * noise
            pi = pi * game.get_valid_moves(state)
            pi = pi / np.sum(pi)
        
        train_examples.append([canonical_board, cur_player, pi, None])
        action = np.random.choice(len(pi), p=pi)
        state = game.get_next_state(state, action)
        r = game.get_game_result(state, cur_player)
        
        if r != 0:
            result = [(x[0], x[2], r * ((-1) ** (x[1] != cur_player))) for x in train_examples]
            # ğŸ”¥ æ¸…ç†æ˜¾å­˜
            del nnet, mcts, state, train_examples
            if args.get('cuda', False) and torch.cuda.is_available():
                torch.cuda.empty_cache()
            return result
        
        new_player = game.get_current_player(state)
        if new_player != cur_player:
            cur_player = new_player


class Coach:
    """
    çœŸæ­£çš„AlphaZeroè®­ç»ƒæµç¨‹:
    1. è‡ªæˆ‘å¯¹å¼ˆæ”¶é›†æ•°æ®
    2. è®­ç»ƒç¥ç»ç½‘ç»œå¾—åˆ°æ–°æ¨¡å‹
    3. Arenaå¯¹æˆ˜: æ–°æ¨¡å‹ vs æ—§æ¨¡å‹
    4. åªæœ‰æ–°æ¨¡å‹èƒœç‡ > é˜ˆå€¼(55%) æ‰æ¥å—æ›´æ–°
    """
    
    def __init__(self, game, nnet, args):
        self.game = game
        self.nnet = nnet
        self.args = args
        self.train_examples_history = []
        
        # ğŸ”§ ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if args.get('cuda', False) and torch.cuda.is_available():
            self.nnet = self.nnet.cuda()
        
        # TensorBoard - å»¶è¿Ÿåˆå§‹åŒ–ï¼Œé¿å…å¤šè¿›ç¨‹åºåˆ—åŒ–é—®é¢˜
        self.log_dir = os.path.join('results', 'logs', 'tensorboard')
        os.makedirs(self.log_dir, exist_ok=True)
        self.writer = None  # åœ¨ learn() ä¸­åˆå§‹åŒ–
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆä¹Ÿè¦åœ¨åŒä¸€è®¾å¤‡ä¸Šï¼‰
        self.best_nnet = copy.deepcopy(nnet)
        if args.get('cuda', False) and torch.cuda.is_available():
            self.best_nnet = self.best_nnet.cuda()
    
    def learn(self):
        """
        AlphaZeroå®Œæ•´è®­ç»ƒå¾ªç¯:
        æ¯æ¬¡è¿­ä»£ = è‡ªæˆ‘å¯¹å¼ˆ â†’ è®­ç»ƒ â†’ Arenaå¯¹æˆ˜ â†’ æ¨¡å‹æ›´æ–°åˆ¤æ–­
        """
        # åˆå§‹åŒ– TensorBoard (ä»…åœ¨ä¸»è¿›ç¨‹)
        if self.writer is None:
            self.writer = SummaryWriter(log_dir=self.log_dir)
            print(f"ğŸ“Š TensorBoard æ—¥å¿—: {self.log_dir}")
            print(f"   å¯åŠ¨å‘½ä»¤: tensorboard --logdir={self.log_dir}\n")
        
        # ç¡®å®šå¹¶è¡Œè¿›ç¨‹æ•°
        num_workers = self.args.get('num_workers', min(cpu_count() - 1, 8))
        use_parallel = self.args.get('use_parallel', True) and num_workers > 1
        
        if use_parallel:
            print(f"ğŸš€ å¯ç”¨å¤šè¿›ç¨‹å¹¶è¡Œ: {num_workers} ä¸ªå·¥ä½œè¿›ç¨‹")
        
        for i in range(1, self.args['num_iterations'] + 1):
            print(f'\n{"=" * 70}')
            print(f'ğŸ“ AlphaZero è¿­ä»£ {i}/{self.args["num_iterations"]}')
            print(f'{"=" * 70}\n')
            
            # ============================================================
            # é˜¶æ®µ1: è‡ªæˆ‘å¯¹å¼ˆæ”¶é›†è®­ç»ƒæ•°æ®
            # ============================================================
            iteration_train_examples = deque([], maxlen=self.args['max_queue_length'])
            
            # é€‰æ‹©è‡ªæˆ‘å¯¹å¼ˆæ¨¡å¼
            self_play_mode = self.args.get('self_play_mode', 'batch')  # 'batch' or 'multiprocess'
            
            if use_parallel and self_play_mode == 'batch':
                # ğŸš€ æ‰¹é‡æ¨ç†æ¨¡å¼ï¼ˆæœ€ä¼˜ï¼‰ï¼šå¤šçº¿ç¨‹å¯¹å±€ + å•GPUæ‰¹é‡æ¨ç†
                from .self_play_batch import self_play_parallel_batch
                train_examples = self_play_parallel_batch(self.game, self.nnet, self.args)
                iteration_train_examples += train_examples
                
            elif use_parallel:
                # å¤šè¿›ç¨‹æ¨¡å¼ï¼ˆæ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹GPUï¼Œæ˜¾å­˜å ç”¨é«˜ï¼‰
                # ğŸ”¥ ç§»åˆ°CPUé¿å…è·¨è¿›ç¨‹ä¼ é€’GPU tensor
                nnet_state = {k: v.cpu() for k, v in self.nnet.state_dict().items()}
                tasks = [
                    (self.game, nnet_state, self.args, np.random.randint(0, 1000000))
                    for _ in range(self.args['num_episodes'])
                ]
                
                with Pool(processes=num_workers, initializer=_init_worker_cuda) as pool:
                    results = list(tqdm(
                        pool.imap(_execute_episode_worker, tasks),
                        total=self.args['num_episodes'],
                        desc=f"ğŸ® è‡ªæˆ‘å¯¹å¼ˆ({num_workers}è¿›ç¨‹)"
                    ))
                    for result in results:
                        iteration_train_examples += result
                
                # ğŸ”¥ è‡ªæˆ‘å¯¹å¼ˆåæ¸…ç†æ˜¾å­˜
                if self.args.get('cuda', False) and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
            else:
                # å•è¿›ç¨‹æ¨¡å¼ (ç›´æ¥ä½¿ç”¨ç°æœ‰ MCTS)
                for _ in tqdm(range(self.args['num_episodes']), desc="ğŸ® è‡ªæˆ‘å¯¹å¼ˆ"):
                    mcts = MCTS(self.game, self.nnet, self.args)
                    train_examples = []
                    state = self.game.get_initial_state()
                    cur_player = 0
                    episode_step = 0
                    
                    while True:
                        episode_step += 1
                        canonical_board = self.game.get_observation(state)
                        temp = int(episode_step < self.args['temp_threshold'])
                        pi = mcts.get_action_prob(state, temp=temp)
                        
                        train_examples.append([canonical_board, cur_player, pi, None])
                        action = np.random.choice(len(pi), p=pi)
                        state = self.game.get_next_state(state, action)
                        r = self.game.get_game_result(state, cur_player)
                        
                        if r != 0:
                            iteration_train_examples += [(x[0], x[2], r * ((-1) ** (x[1] != cur_player))) for x in train_examples]
                            break
                        
                        new_player = self.game.get_current_player(state)
                        if new_player != cur_player:
                            cur_player = new_player
            
            self.train_examples_history.append(iteration_train_examples)
            if len(self.train_examples_history) > self.args['num_iters_for_train_examples_history']:
                self.train_examples_history.pop(0)
            
            train_examples = []
            for e in self.train_examples_history:
                train_examples.extend(e)
            
            print(f"âœ“ æ”¶é›†åˆ° {len(train_examples)} ä¸ªè®­ç»ƒæ ·æœ¬\n")
            
            # ============================================================
            # é˜¶æ®µ2: è®­ç»ƒç¥ç»ç½‘ç»œ
            # ============================================================
            self.train(train_examples, iteration=i)
            
            # ============================================================
            # é˜¶æ®µ3: Arenaå¯¹æˆ˜ - æ–°æ¨¡å‹ vs å†å²æœ€å¥½æ¨¡å‹ (æ¯Næ¬¡è¿­ä»£è¿›è¡Œä¸€æ¬¡)
            # ============================================================
            arena_interval = self.args.get('arena_interval', 1)  # é»˜è®¤æ¯æ¬¡éƒ½éªŒè¯
            should_arena = (i % arena_interval == 0) or (i == self.args['num_iterations'])
            
            if should_arena:
                # ğŸ”¥ Arena å‰æ¸…ç†æ˜¾å­˜
                if self.args.get('cuda', False) and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                
                print(f"\nğŸ¥Š Arenaå¯¹æˆ˜éªŒè¯ (è¿­ä»£ {i}): æ–°è®­ç»ƒæ¨¡å‹ vs å†å²æœ€å¥½æ¨¡å‹")
                
                # ğŸ”¥ ä½¿ç”¨å†å²æœ€å¥½æ¨¡å‹ä½œä¸ºå¯¹æ‰‹ (ç§»åˆ°CPUé¿å…è·¨è¿›ç¨‹ä¼ é€’GPU tensor)
                best_nnet_state = {k: v.cpu() for k, v in self.best_nnet.state_dict().items()}
                
                # å½“å‰æ¨¡å‹state_dict (ç§»åˆ°CPU)
                current_state = {k: v.cpu() for k, v in self.nnet.state_dict().items()}
                
                # é€‰æ‹©Arenaå®ç°
                # ğŸ¯ é»˜è®¤ä½¿ç”¨ gpu_multiprocess æ¨¡å¼ï¼šçœŸæ­£çš„å¤šè¿›ç¨‹å¹¶è¡Œ
                arena_mode = self.args.get('arena_mode', 'gpu_multiprocess')  # 'gpu_multiprocess', 'gpu_thread', 'multiprocess'
                
                if arena_mode == 'gpu_multiprocess':
                    # ğŸš€ GPUå¤šè¿›ç¨‹æ¨¡å¼ï¼ˆæ¨èï¼‰ï¼šçœŸæ­£çš„å¤šæ ¸å¹¶è¡Œ + GPUåŠ é€Ÿ
                    # ä¼˜ç‚¹ï¼šå……åˆ†åˆ©ç”¨å¤šæ ¸CPUã€GPUåŠ é€Ÿã€ä¸è‡ªæˆ‘å¯¹å¼ˆåŒæ ·æ–¹å¼
                    from .arena_gpu_multiprocess import ArenaGPUMultiProcess
                    arena = ArenaGPUMultiProcess(self.nnet, self.best_nnet, self.game, self.args)
                elif arena_mode == 'gpu_thread':
                    # GPUå¤šçº¿ç¨‹æ¨¡å¼ï¼šå—GILé™åˆ¶ï¼Œå®é™…åªç”¨1-2ä¸ªæ ¸å¿ƒ
                    from .arena_gpu import ArenaGPU
                    arena = ArenaGPU(self.nnet, self.best_nnet, self.game, self.args)
                elif arena_mode == 'batch':
                    # æ‰¹é‡æ¨ç†æ¨¡å¼ï¼ˆå®éªŒæ€§ï¼‰ï¼šç›®å‰fallbackåˆ°å¤šè¿›ç¨‹
                    from .arena_batch_inference import ArenaBatchInference
                    arena = ArenaBatchInference(self.nnet, self.best_nnet, self.game, self.args)
                else:
                    # CPUå¤šè¿›ç¨‹æ¨¡å¼ï¼ˆæ…¢ä½†ç¨³å®šï¼‰ï¼šç”¨äºè°ƒè¯•æˆ–æ— GPUç¯å¢ƒ
                    arena = Arena(current_state, best_nnet_state, self.game, self.args)
                
                new_wins, old_wins, draws = arena.play_games(self.args['arena_compare'])
                
                # ğŸ”¥ ç«‹å³é‡Šæ”¾ Arena å’Œæ˜¾å­˜
                del arena
                if self.args.get('cuda', False) and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    # å¼ºåˆ¶åŒæ­¥ï¼Œç¡®ä¿æ˜¾å­˜çœŸæ­£é‡Šæ”¾
                    torch.cuda.synchronize()
                
                # è®¡ç®—æ–°æ¨¡å‹èƒœç‡
                total_games = new_wins + old_wins + draws
                new_win_rate = (new_wins + 0.5 * draws) / total_games
                
                print(f"\nğŸ“Š æ–°æ¨¡å‹èƒœç‡: {new_win_rate*100:.1f}% ({new_wins}èƒœ {draws}å¹³ {old_wins}è´Ÿ)")
                
                # è®°å½•åˆ° TensorBoard
                self.writer.add_scalar('Arena/win_rate', new_win_rate, i)
                self.writer.add_scalar('Arena/new_wins', new_wins, i)
                self.writer.add_scalar('Arena/old_wins', old_wins, i)
                self.writer.add_scalar('Arena/draws', draws, i)
                
                # ============================================================
                # é˜¶æ®µ4: æ¨¡å‹æ›´æ–°åˆ¤æ–­
                # ============================================================
                threshold = self.args.get('update_threshold', 0.55)
                
                if new_win_rate >= threshold:
                    print(f'âœ… æ–°æ¨¡å‹èƒœç‡ {new_win_rate*100:.1f}% >= {threshold*100:.1f}% â†’ æ¥å—æ›´æ–°!')
                    self.best_nnet = copy.deepcopy(self.nnet)
                    self.save_checkpoint(filename=f'best_{i}.pth')
                    self.writer.add_scalar('Arena/model_accepted', 1, i)
                else:
                    print(f'âŒ æ–°æ¨¡å‹èƒœç‡ {new_win_rate*100:.1f}% < {threshold*100:.1f}% â†’ best_nnetä¿æŒä¸å˜')
                    print(f'   ç»§ç»­ç”¨æ–°è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œä¸‹ä¸€è½®è‡ªæˆ‘å¯¹å¼ˆ...')
                    self.writer.add_scalar('Arena/model_accepted', 0, i)
            else:
                print(f"\nâ­ï¸  è·³è¿‡ArenaéªŒè¯ (ä¸‹æ¬¡éªŒè¯: è¿­ä»£ {(i // arena_interval + 1) * arena_interval})")
                # ä¸éªŒè¯æ—¶ï¼Œç›´æ¥æ¥å—æ–°æ¨¡å‹
                self.best_nnet = copy.deepcopy(self.nnet)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if i % self.args['checkpoint_interval'] == 0:
                self.save_checkpoint(filename=f'checkpoint_{i}.pth')
            self.save_checkpoint(filename='latest.pth')
        
        # è®­ç»ƒç»“æŸï¼Œå…³é—­ TensorBoard writer
        if self.writer is not None:
            self.writer.close()
            print("\nğŸ“Š TensorBoard æ—¥å¿—å·²ä¿å­˜")
    
    def train(self, examples, iteration=0):
        """è®­ç»ƒç¥ç»ç½‘ç»œ"""
        optimizer = optim.Adam(
            self.nnet.parameters(), 
            lr=self.args['lr'], 
            weight_decay=self.args['weight_decay']
        )
        self.nnet.train()
        
        # é¢„å…ˆæ‰“ä¹±æ•°æ®
        np.random.shuffle(examples)
        num_batches = len(examples) // self.args['batch_size']
        all_indices = np.arange(len(examples))
        
        print(f"\nğŸ§  è®­ç»ƒç¥ç»ç½‘ç»œ: {self.args['epochs']} epochs, {num_batches} batches/epoch")
        
        # ä½¿ç”¨ tqdm åŒ…è£…æ•´ä¸ª epoch å¾ªç¯
        epoch_iter = tqdm(range(self.args['epochs']), desc='ğŸ¯ è®­ç»ƒè¿›åº¦', unit='epoch')
        
        for epoch in epoch_iter:
            np.random.shuffle(all_indices)
            
            epoch_pi_loss = 0
            epoch_v_loss = 0
            epoch_start = time.time()
            
            batch_iter = range(num_batches)  # ä¸æ˜¾ç¤º batch è¿›åº¦ï¼Œåªæ˜¾ç¤º epoch
            
            for batch_idx in batch_iter:
                start_idx = batch_idx * self.args['batch_size']
                end_idx = start_idx + self.args['batch_size']
                batch_indices = all_indices[start_idx:end_idx]
                
                boards, pis, vs = list(zip(*[examples[i] for i in batch_indices]))
                
                boards = torch.FloatTensor(np.array(boards))
                target_pis = torch.FloatTensor(np.array(pis))
                target_vs = torch.FloatTensor(np.array(vs))
                
                if self.args['cuda']:
                    boards = boards.cuda()
                    target_pis = target_pis.cuda()
                    target_vs = target_vs.cuda()
                
                # å‰å‘ä¼ æ’­
                out_pi, out_v = self.nnet(boards)
                l_pi = -torch.sum(target_pis * out_pi) / target_pis.size()[0]
                l_v = torch.sum((target_vs - out_v.view(-1)) ** 2) / target_vs.size()[0]
                total_loss = l_pi + l_v
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(self.nnet.parameters(), max_norm=1.0)
                optimizer.step()
                
                epoch_pi_loss += l_pi.item()
                epoch_v_loss += l_v.item()
            
            # è®¡ç®—å¹¶æ›´æ–°æŸå¤±åˆ° tqdm
            avg_pi_loss = epoch_pi_loss / num_batches
            avg_v_loss = epoch_v_loss / num_batches
            total_loss = avg_pi_loss + avg_v_loss
            epoch_time = time.time() - epoch_start
            
            # è®°å½•åˆ° TensorBoard
            global_step = iteration * self.args['epochs'] + epoch
            self.writer.add_scalar('Loss/policy', avg_pi_loss, global_step)
            self.writer.add_scalar('Loss/value', avg_v_loss, global_step)
            self.writer.add_scalar('Loss/total', total_loss, global_step)
            self.writer.add_scalar('Training/speed_batches_per_sec', num_batches/epoch_time, global_step)
            
            # æ›´æ–° tqdm æ˜¾ç¤º
            epoch_iter.set_postfix({
                'Ï€_loss': f'{avg_pi_loss:.3f}',
                'v_loss': f'{avg_v_loss:.3f}',
                'total': f'{total_loss:.3f}',
                'speed': f'{num_batches/epoch_time:.1f}b/s'
            })
    
    def save_checkpoint(self, filename='checkpoint.pth'):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        os.makedirs(self.args['checkpoint'], exist_ok=True)
        filepath = os.path.join(self.args['checkpoint'], filename)
        torch.save({'state_dict': self.nnet.state_dict()}, filepath)
        if 'best' in filename:
            print(f'ğŸ† æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {filepath}')
        else:
            print(f'ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filepath}')
