# -*- coding: utf-8 -*-
"""è‡ªæˆ‘å¯¹å¼ˆ - æ‰¹é‡æ¨ç†ç‰ˆæœ¬ï¼ˆç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼ï¼‰"""

import numpy as np
import torch
from tqdm import tqdm
from .mcts_batch import MCTSBatchInference
from .batch_inference_server import BatchInferenceServer
from concurrent.futures import ThreadPoolExecutor, as_completed


def execute_episode_batch(game, nnet, args, inference_server, model_idx=0):
    """
    æ‰§è¡Œä¸€å±€è‡ªæˆ‘å¯¹å¼ˆï¼ˆä½¿ç”¨æ‰¹é‡æ¨ç†ï¼‰
    
    Args:
        game: æ¸¸æˆå®ä¾‹
        nnet: ç¥ç»ç½‘ç»œï¼ˆä»…ç”¨äºè·å–å‚æ•°ä¿¡æ¯ï¼Œä¸ç›´æ¥æ¨ç†ï¼‰
        args: é…ç½®å‚æ•°
        inference_server: æ‰¹é‡æ¨ç†æœåŠ¡å™¨
        model_idx: æ¨¡å‹ç´¢å¼•ï¼ˆæ€»æ˜¯0ï¼Œå› ä¸ºè‡ªæˆ‘å¯¹å¼ˆåªç”¨ä¸€ä¸ªæ¨¡å‹ï¼‰
    
    Returns:
        è®­ç»ƒæ ·æœ¬åˆ—è¡¨
    """
    mcts = MCTSBatchInference(game, nnet, args)
    mcts.set_inference_server(inference_server, model_idx)
    
    train_examples = []
    state = game.get_initial_state()
    cur_player = 0
    episode_step = 0
    
    while True:
        episode_step += 1
        canonical_board = game.get_observation(state)
        temp = int(episode_step < args['temp_threshold'])
        pi = mcts.get_action_prob(state, temp=temp)
        
        # å‰30æ­¥æ·»åŠ Dirichletå™ªå£°å¢åŠ æ¢ç´¢
        if episode_step <= 30:
            noise = np.random.dirichlet([args.get('dirichlet_alpha', 0.3)] * len(pi))
            pi = (1 - args.get('dirichlet_epsilon', 0.25)) * pi + args.get('dirichlet_epsilon', 0.25) * noise
            pi = pi * game.get_valid_moves(state)
            pi = pi / np.sum(pi)
        
        train_examples.append([canonical_board, cur_player, pi, None])
        action = np.random.choice(len(pi), p=pi)
        state = game.get_next_state(state, action)
        r = game.get_game_result(state, cur_player)
        
        if r != 0:
            # æ¸¸æˆç»“æŸï¼Œè¿”å›å¸¦æ ‡ç­¾çš„è®­ç»ƒæ ·æœ¬
            return [(x[0], x[2], r * ((-1) ** (x[1] != cur_player))) for x in train_examples]
        
        new_player = game.get_current_player(state)
        if new_player != cur_player:
            cur_player = new_player


def self_play_parallel_batch(game, nnet, args):
    """
    å¹¶è¡Œè‡ªæˆ‘å¯¹å¼ˆï¼ˆç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼ï¼‰
    
    âš ï¸ é—®é¢˜ï¼šPython GILå¯¼è‡´å¤šçº¿ç¨‹æ— æ³•åˆ©ç”¨å¤šæ ¸CPU
    è§£å†³ï¼šæ”¹å›å¤šè¿›ç¨‹ï¼Œä½†æ¯ä¸ªè¿›ç¨‹å…±äº«GPUæ¨ç†æœåŠ¡
    
    æš‚æ—¶fallbackåˆ°åŸå§‹å¤šè¿›ç¨‹ç‰ˆæœ¬
    """
    num_episodes = args['num_episodes']
    num_workers = args.get('num_workers', 10)
    
    print(f"\nâš ï¸  æ‰¹é‡æ¨ç†æ¨¡å¼å—GILé™åˆ¶ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å¤šè¿›ç¨‹æ¨¡å¼")
    print(f"   æ¶æ„: {num_workers} ä¸ªCPUè¿›ç¨‹ï¼ˆå„è‡ªGPUæ¨ç†ï¼‰")
    print(f"   æ˜¾å­˜: {num_workers}ä¸ªæ¨¡å‹ (~{num_workers*200}MB)")
    
    # Fallbackåˆ°åŸå§‹å¤šè¿›ç¨‹å®ç°
    from multiprocessing import Pool
    from .coach_alphazero import _execute_episode_worker
    
    nnet_state = nnet.state_dict()
    tasks = [
        (game, nnet_state, args, np.random.randint(0, 1000000))
        for _ in range(num_episodes)
    ]
    
    all_train_examples = []
    with Pool(processes=num_workers) as pool:
        results = list(tqdm(
            pool.imap(_execute_episode_worker, tasks),
            total=num_episodes,
            desc=f"ğŸ® è‡ªæˆ‘å¯¹å¼ˆ({num_workers}è¿›ç¨‹)"
        ))
        for result in results:
            all_train_examples.extend(result)
    
    print(f"âœ“ æ”¶é›†åˆ° {len(all_train_examples)} ä¸ªè®­ç»ƒæ ·æœ¬")
    
    return all_train_examples
